defaults:
  seed: 0

# specifics about experiment that will be logged to telegram
description: "Train model in several stages with different resolutions, augmentations, losses, optimizers and schedulers"


########################################################
################### Dataset settings ###################
########################################################

datasets:
  # Steps per epoch for each dataset. Total steps = steps_per_epoch * len(datasets.list)
  steps_per_epoch: 10

  list:
    - id: 0     # Used to identify dataset in training stages
      name: human                        # Used to get NameDataset from data/name_dataset.py
      root: '/Users/vladbahteev/Human-Segmentation-Dataset'
      use_to_train: True
      use_to_validate: True

      pre_transforms: []
      augmentations:
        - name: HorizontalFlip
        - name: ShiftScaleRotate
      post_transforms: [ { 'name': 'Resize', 'height': 128, 'width': 128 }, { 'name': 'ToTensor' } ]



dataloader:
  num_workers: 0
  batch_size: 10
  pin_memory: True

########################################################
#################### Model settings ####################
########################################################

model:
  name: 'Unet'
  n_dim: 2
  pipeline: 'segmentation'      # [segmentation]
  params: {'encoder_name': 'resnet18', 'in_channels': 3, 'classes': 1}

criterion: [{ 'cls': 'pytorch_toolbelt.losses.FocalLoss', 'gamma': 3 }]

optimizer: [{'cls': 'torch.optim.Adam', 'lr': 0.001}]

# For some schedulers, you need to rewrite the place where it is called
scheduler: [{
  'cls': 'torch.optim.lr_scheduler.CosineAnnealingWarmRestarts',
  'T_0': 2, 'T_mult': 2
}]

########################################################
################## Training settings ###################
########################################################

callbacks: [
  {'name': 'ModelCheckpoint', 'monitor': 'intersection_over_union', 'mode': 'max', 'save_last': False, 'save_top_k': 1},
  {'name': 'EarlyStopping', 'monitor': 'valid_loss', 'mode': 'min', 'patience': 15},
  ]

logging:
  name: null   # [null | csv_logs | mlflow | neptune | comet | tensorboard | test_tube | swandb]
  params: {}
  metrics: [
    {'name': 'intersection_over_union', 'threshold': 0.5}
  ]

# You can add any argument for pytorch_lightning.Trainer
lightning:
  gpus: 0
  num_nodes: 1

# Parameters defined in stages are used to update fields defined above
train_stages:
  - name: res-128-pretraining
    lightning:
      max_epochs: 2


  - name: res-256-training
    lightning:
      max_epochs: 1

    # Bigger resolution may need to make smaller batch size
    dataloader:
      batch_size: 5

    datasets:
      list:
        - id: 0

          # Add Blur and GaussNoise to augmentations
          augmentations:
            - name: OneOf
              list:
                - name: GaussNoise
                - name: Blur
            - name: HorizontalFlip
            - name: ShiftScaleRotate

          # Change size of result image
          post_transforms: [
            {'name': 'Resize', 'height': 256, 'width': 256},
            {'name': 'ToTensor'},
          ]

    # Change loss function
    criterion: [ {
      'cls': 'pytorch_toolbelt.losses.JointLoss',
      'first_weight': 1.0,
      'second_weight': 2.0,
      'first': { 'cls': 'pytorch_toolbelt.losses.DiceLoss', 'mode': 'binary' },
      'second': { 'cls': 'pytorch_toolbelt.losses.FocalLoss', 'gamma': 3 }
    } ]


  - name: fine-tuning
    lightning:
      max_epochs: 2

    # SGD more accurate at last steps
    optimizer: [{'cls': 'torch.optim.SGD', 'lr': 1e-4}]

    # Change to less aggressive scheduler
    scheduler: [{'cls': 'torch.optim.lr_scheduler.ReduceLROnPlateau', 'factor': 0.2, 'patience': 3}]